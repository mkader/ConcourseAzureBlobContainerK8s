---
# Generating and storing a tarball (5., 6. & 7.)
# ==========================================
# 5. declare resource types
# ==========================
# Not every platform wants to run containers. Some platforms happily take your source code and run it. 
# So our pipeline should also generate a single artifact with all the published ASP.NET Core files.
# I wanted to store this blob in Azure Storage. Since Azure Storage isn't a built-in Concourse resource type, 
# For non-core resources, you have to declare the resource type in the pipeline YAML.
# A resource type declaration is fairly simple; it's just a type (often docker-image) and then the repo to get it from.
resource_types:
- name: azure-blobstore
  type: docker-image
  source:
    repository: pcfabr/azure-blobstore-resource

# Generating AKS credentials (11, 12, 13, 14, 15, 16)
# ============================
# Recall, we set up a basic AKS cluster. For Concourse to talk to AKS, we need credentials!
# From within the Azure Portal, I started up an instance of the Cloud Shell. This is a hosted Bash environment with lots of pre-loaded tools. 
# From here, I used the AKS CLI to get the administrator credentials for my cluster.
# ervice principal id/secret => 97c55954-4445-4c30-a104-bf21af88b954 =>  rl[bsIhm2Ij9d27W_xBD?ZVHfyDvY-..
# This command generated a configuration file with URLs, users, certificates, and tokens. I copied this file locally for use later in my pipeline. cat .kube/config
# az aks get-credentials --name vs-k8s-cluster --resource-group VSP_Resource_Group --admin
# 11.Creating a role-binding for permission to deploy
# ================================================
# https://docs.bitnami.com/kubernetes/how-to/configure-rbac-in-your-kubernetes-cluster/
# The administrative user doesn't automatically have rights to do much in the default cluster namespace. 
# Without explicitly allowing permissions, you'll get some gnarly "does not have access" errors when doing most anything. 
# Enter role-based access controls. I created a new rolebinding named "admin" with admin rights in the cluster, and mapped to the existing clusterAdmin user.
# Now I knew that Concourse could effectively interact with my Kubernetes cluster.
# kubectl create rolebinding admin --clusterrole=admin --user=clusterAdmin --namespace=default
# 11.1 Grant AKS access to ACR
# =======================
# AKS_RESOURCE_GROUP=VSP_Resource_Group
# AKS_CLUSTER_NAME=vs-k8s-cluster
# ACR_RESOURCE_GROUP=VSP_Resource_Group
# ACR_NAME=FTMyRepository

# Get the id of the service principal configured for AKS
# CLIENT_ID=$(az aks show --resource-group $AKS_RESOURCE_GROUP --name $AKS_CLUSTER_NAME --query "servicePrincipalProfile.clientId" --output tsv)

# # Get the ACR registry resource id
# ACR_ID=$(az acr show --name $ACR_NAME --resource-group $ACR_RESOURCE_GROUP --query "id" --output tsv)

# # Create role assignment
# az role assignment create --assignee $CLIENT_ID --role acrpull --scope $ACR_ID

# 11.2 Access with Kubernetes secret
# ==============================
# kubectl create secret docker-registry acr-auth --docker-server ftmyrepository.azurecr.io --docker-username FTMyRepository --docker-password 7TwDC=wDsMg0v8NFvARN2juln1mPdREE --docker-email mohideen.kader@titletec.com

# 12.Giving AKS access to Azure Container Registry (ACR)
# =============================================
# https://docs.microsoft.com/en-us/azure/container-registry/container-registry-auth-aks
# Right now, ACR doesn't support an anonymous access strategy. Everything happens via authenticated users. 
# The Kubernetes cluster needs access to its container registry, so I followed these instructions to connect ACR to AKS. Pretty easy!
# 13. Creating Kubernetes deployment and service definitions
# ======================================================
# https://kubernetes.io/docs/concepts/workloads/controllers/deployment/
# https://kubernetes.io/docs/concepts/services-networking/service/
# Concourse is going to apply a Kubernetes deployment to create pods of containers in the cluster. 
# Then, Concourse will apply a Kubernetes service to expose my pod with a routable endpoint.
# I created a pair of configurations and added them to the ci folder of my source code. The deployment.yaml looks like:
# This is a pretty basic deployment definition. It points to the latest image in the ACR and deploys a single instance (replicas: 1).
# My service.yaml is also fairly simple, and AKS will provision the necessary Azure Load Balancer and public IP addresses.
# 14.Adding Kubernetes resource definitions to the Concourse pipeline
# ===================================================================
# https://github.com/zlabjp/kubernetes-resource
# First, I added a new resource type to the Concourse pipeline. Because Kubernetes isn't a baked-in resource type. 
# It's important than the Kubernetes client and server are expecting the same Kubernetes version, so I set the tag to match my AKS version.
- name: kubernetes
  type: docker-image
  source:
    repository: zlabjp/kubernetes-resource
    tag: "1.13"

resources:
# 1. Pulling source code
# ======================
# https://concourse.github.io/dutyfree/
# declare resources, Gave the resource a name ("source-code") and identified where the code lives. 
# when i deploy a pipeline, Concourse produces containers that "check" resources on a schedule for any changes that should trigger a pipeline.
- name: source-code
  type: git
  icon: github-circle
  source:
    uri: {{source-code-uri}}
    branch: master

# 3. Producing and publishing a container image (3. & 4.)
# ========================================================
# A pipeline that just run tests is kinda weird. I need to do something when tests pass. In my case, I wanted to generate a Docker image. 
# Another of the built-in Concourse resource types is "docker-image" which generates a container image and puts it into a registry. 
# Here's the resource definition that worked with Azure Container Registry:
# username: ((azure-registry-username)), password: ((azure-registry-password))
# From the Azure Portal => Azure Container Registry => "Access keys" => grabbed the Username and one of the passwords.
- name: azure-container-registry
  type: docker-image
  icon: docker
  source:
    repository: {{azure-container-registry-repository}}
    tag: latest
    username: {{azure-container-registry-username}}
    password: {{azure-container-registry-password}}

# 6. created for Azure Storage standard resource definition:
# ===========================================================
# https://github.com/pivotal-cf/azure-blobstore-resource
# storage_account_name: ((azure-storage-account-name)), storage_account_key: ((azure-storage-account-key)), container: ftcoreapp
# Here the "type" matches the resource type name I set earlier. 
# Then I set the credentials (retrieved from the "Access keys" section in the Azure Portal), container name (pre-created, 
# and the name of the file to upload. Regex is supported here too.
- name: azure-blobstore
  type: azure-blobstore
  icon: azure
  source:
    storage_account_name: {{azure-blobstore-storage-account-name}}
    storage_account_key: {{azure-blobstore-storage-account-key}}
    container: {{azure-blobstore-container-name}}
    versioned_file: {{azure-blobstore-versioned-file}}

# Adding semantic version to the container image (8., 9. & 10. )
# ======================================================
# I could stop there and push to Kubernetes (next file), but I wanted to do one more thing. 
# I don't like publishing Docker images with the "latest" tag. I want a real version number. 
# It makes sense for many reasons, not the least of which is that Kubernetes won't pick up changes to a container if the tag doesn't change! 
# Fortunately, Concourse has a default resource type for semantic versioning.
# There are a few backing stores for the version number. Since Concourse is stateless, 
# we need to keep the version value outside of Concourse itself. I chose a git backend. 
# Specifically, I added a branch named "version" to my GitHub repo, and added a single file (no extension) named "version". 
# I started the version at 0.1.0.
# Then, I ensured that my GitHub account had an SSH key associated with it. 
# I needed this so that Concourse could write changes to this version file sitting in GitHub.

# 9. Generating a new SSH key and adding it to the ssh-agent, https://help.github.com/en/articles/connecting-to-github-with-ssh
# =============================================================================================================================
# 1. Open Git Bash. (type here to search)
# 2. Paste the text below, substituting in your GitHub email address. $ ssh-keygen -t rsa -b 4096 -C "daynightsoft@gmail.com"
#      This creates a new ssh key, using the provided email as a label.
#      > Generating public/private rsa key pair.
#      When you're prompted to "Enter a file in which to save the key," press Enter. This accepts the default file location.
# 3.> Enter a file in which to save the key (/c/Users/you/.ssh/id_rsa):[Press enter]
# 4.> Enter passphrase (empty for no passphrase): [Type a passphrase]  (i didn't enter any password)
#   > Enter same passphrase again: [Type passphrase again]
# 5. Ensure the ssh-agent is running: $ eval $(ssh-agent -s)
# 6. Add your SSH private key to the ssh-agent. $ ssh-add ~/.ssh/id_rsa
# 7. Copy the SSH key to your clipboard. $ clip < ~/.ssh/id_rsa.pub
# 8. Github.com->Settings->SSH and GPG keys->New SSH key or Add SSH key

# 8.I added a new resource to my pipeline definition, referencing the built-in semver resource type.
# ===================================================================================================
# In that resource definition, I pointed at the repo URI, branch, file name, and embedded the private key for my account.
# this works:, Place the private key in gitkey.yml,  fly -t rs set-pipeline -c azure-k8s-rev4.yml -p azure-k8s-rev4 --load-vars-from "gitkey.yml"
# This doesn't work: # fly -t rs set-pipeline -c azure-k8s-rev4.yml -p azure-k8s-rev4 --var "github-private-key=$(cat .ssh/id_rsa)"
- name: version  
  type: semver
  source:
    driver: git
    uri: {{github-git-uri}}
    branch: {{github-git-branch}}
    file: {{github-git-file}}
    private_key: {{github-private-key}}

# 15.Next, I had to declare my resource itself. It has references to the credentials we generated earlier.
# ========================================================================================================
# https://www.base64decode.org/
# There are a few key things to note here. First, the "server" refers to the cluster DNS server name in the credentials file. 
# The "token" refers to the token associated with the clusterAdmin user. For me, it's the last "user" called out in the credentials file. 
# Finally, let's talk about the certificate authority. This value comes from the "certificate-authority-data" entry associated with the cluster DNS server. 
# HOWEVER, this value is base64 encoded, and I needed a decoded value. So, I decoded it, and embedded it as you see above.
# certificate_authority: |  
# -----BEGIN CERTIFICATE-----
#     [...]
#  -----END CERTIFICATE-----
- name: azure-kubernetes-service
  type: kubernetes
  icon: azure
  source:
    server: {{azure-kubernetes-service-server}}
    namespace: default
    token: {{azure-kubernetes-service-token}}
    certificate_authority: {{azure-certificate_authority}}
    
# list out jobs
jobs:
# 2. first job Running unit tests   
# =========================
# defined a job to execute unit tests.
# A job has a build plan. That build plan contains any of three things: 1. get steps (to retrieve a resource), 
# 2. put steps (to push something to a resource), 3.and task steps (to run a script). 
# Our job below has one get step (to retrieve source code), and one task (to execute the xUnit tests).

# Let's break it down. First, my "plan" gets the source-code resource, 
# then I set "trigger: true" Concourse will kick off this job whenever it detects a change in the source code.
# Next, my build plan has a "task" step. Tasks run in containers, so you need to choose a base image that runs the user-defined script. 
# I chose the MS-provided .NET Core image so that I'd be confident it had all the necessary .NET tooling installed. 
# Note that my task has an "input." Since tasks are like functions, they have inputs and outputs. 
# Anything I input into the task is mounted into the container and is available to any scripts. 
# So, by making the source-code an input, my shell script can party on the source code retrieved by Concourse.
# Finally, I embedded a short script that invokes the "dotnet test" command. 

# From the fly CLI, I deploy pipelines with the following command: fly -t rs set-pipeline -c azure-k8s-rev1.yml -p azure-k8s-rev1
# That command says to use the "rs" target (which points to a given Concourse instance), 
# use the YAML file holding the pipeline, and name this pipeline azure-k8s-rev1. 
# It deployed instantly, and looked like this in the Concourse web dashboard.
# piepline is [source-code]-----[run-unit-tests]

# After unpausing the pipeline so that it came alive, I saw the "run unit tests" job start running. 
# It's easy to view what a job is doing, and I saw that it loaded the container image from Microsoft, 
# mounted the source code, ran my script and turned "green" because all my tests passed.
- name: run-unit-tests
  plan:
  - get: source-code
    trigger: true
  - task: first-task
    config: 
      platform: linux
      image_resource:
        type: docker-image
        source: 
            repository: mcr.microsoft.com/dotnet/core/sdk
            tag: {{dotnet-core-sdk-tag}}
      inputs:
      - name: source-code
      run:
          path: sh
          args:
          - -exec
          - |
            dotnet test {{source-code-test-proj-path}} 

# 4. second job produces a container
# ==================================
# Notice that I "get" the source code again. I also set a "passed" attribute meaning this will only run if the unit test step completes successfully. 
# This is how you start chaining jobs together into a pipeline! Then I "put" into the registry. 
# Recall I generated a Dockerfile from within VSc, and here, I point to it. 
# The resource does a "docker build" with that Dockerfile, tags the resulting image as the "latest" one, and pushes to the registry.
# piepline is [source-code]-----[run-unit-tests]-----[source-code]-----[containerize-app]-----[azure-container-registry]
# After it completed, the "containerize app" job ran. When that was finished, I checked Azure Container Registry and saw a new repository one with image in it.

# 10. Next, I updated the existing "containerization" job to get the version resource, use it, and then update it.
# ================================================================================================================
# First, I added another "get" for version. Notice that its parameter increments the number by one minor version. 
# Then, see that the "put" for the container registry uses "version/version" as the tag file. 
# This ensures our Docker image is tagged with the semantic version number. 
# Finally, notice I "put" the incremented version file back into GitHub after using it successfully.
# piepline is [source-code]-----[run-unit-tests]-----[source-code]-----[containerize    ]-----[azure-container-registry] 
#                                                      | [version]-----[            -app]-----[version]
#                                                      V---------------[package-app]-----[azure-blobstore]   
# With the pipeline done, I saw that the "version" value in GitHub was incremented by the pipeline, 
# and most importantly, our Docker image has a version tag.
# not working fly -t rs set-pipeline -c azure-k8s-rev4.yml -p azure-k8s-rev4 --var "github-private-key=$(cat .ssh/id_rsa)"
# this works:, Place the private key in gitkey.yml, fly -t rs set-pipeline -c azure-k8s-final.yml -p azure-k8s-final --load-vars-from "gitkey.yml" --load-vars-from "azurecertificate.yml"
- name: containerize-app
  plan:
  - get: source-code
    trigger: true
    passed:
    - run-unit-tests
  - get: version
    params: {bump: minor}
  - put: azure-container-registry
    params:
      build: ./source-code
      tag_file: ((github-git-branch))/((github-git-file))
      tag_as_latest: true
  - put: version
    params: {file: ((github-git-branch))/((github-git-file))}

# 7. third job bundles code and uploads it
# ========================================
# Finally, I added a new job that takes source code, runs a "publish" command, and creates a tarball from the result.
# Note that this job is also triggered when unit tests succeed. But it's not connected to the containerization job, so it runs in parallel. 
# Also note that in addition to an input, I also have outputs defined on the task. 
# This generates folders that are visible to subsequent steps in the job. 
# I dropped the tarball into the "artifact-repo" folder, and then "put" that file into Azure Blob Storage
# piepline is [source-code]-----[run-unit-tests]-----[source-code]-----[containerize-app]-----[azure-container-registry]
#                                                               V------[package-app]-----[azure-blobstore]    
#I After the pipeline finished, I had another updated container image in Azure Container Registry and a file sitting in Azure Storage.
- name: package-app
  plan:
  - get: source-code
    trigger: true
    passed:
    - run-unit-tests
  - task: first-task
    config:
      platform: linux
      image_resource:
        type: docker-image
        source: 
            repository: mcr.microsoft.com/dotnet/core/sdk
            tag: {{dotnet-core-sdk-tag}}
      inputs:
      - name: source-code
      outputs:
      - name: compiled-app
      - name: artifact-repo
      run:
          path: sh
          args:
          - -exec
          - |
            dotnet publish {{source-code-api-proj-path}} -o .././compiled-app
            tar -czvf ./artifact-repo/{{azure-blobstore-versioned-file}} ./compiled-app
            ls
  - put: azure-blobstore
    params:
      file: artifact-repo/((azure-blobstore-versioned-file)) 
      
# 16. fourth job deploys to Kubernetes
# ====================================
# First, I "get" the ACR resource. When it changes (because it gets a new version of the container), it triggers this job. 
# It only fires if the "containerize app" job passes first. Then I get the source code (so that I can grab the deployment.yaml and service.yaml files I put in the ci folder), 
# and I get the semantic version.
# Next I "put" to the AKS resource, twice. In essence, this resource executes kubectl commands. 
# The first command does a kubectl apply for both the deployment and service. 
# On the first run, it provisions the pod and exposes it via a service. However, because the container image tag in the deployment file is to "latest", 
# Kubernetes actually won't retrieve new images with that tag after I apply a deployment. 
# So, I "patched" the deployment in a second "put" step and set the deployment's image tag to the semantic version. This triggers a pod refresh!
# fly -t rs set-pipeline -c azure-k8s-final.yml -p azure-k8s-final --load-vars-from "gitkey.yml" "azurecertificate.yml"
# piepline is [source-code]-----[run-unit-tests]-----[source-code]-----[containerize    ]-----[azure - container - registry]-----[deploy    ]----[azure-kubernetes-service] 
#                                                      | [version]-----[            -app]-----[version]            [version]-----[   loy    ] 
#                                                      V---------------[package-app]-----[azure-blobstore]     [source-code]-----[      -app]
# did it actually work? In Azure Cloud Shell, I ran a "kubectl get pods" and "kubectl get services" command. 
# The first showed our running pod, and the second showed the external IP assigned to my pod.- name: deploy-app
#   mohideen_kader@Azure:~$ kubectl get pods
#   NAME                       READY   STATUS    RESTARTS   AGE
#   demo-app-86dddfb97-w9wzc   1/1     Running   0          15h
#   mohideen_kader@Azure:~$ kubectl get services
#   NAME         TYPE           CLUSTER-IP    EXTERNAL-IP      PORT(S)        AGE
#   demo-app     LoadBalancer   10.0.131.51   137.116.253.17   80:32045/TCP   17h
#   kubernetes   ClusterIP      10.0.0.1      <none>           443/TCP        21h
# I also issued a request to that URL in the browser, and got back my ASP.NET Core API results. http://137.116.253.17api/values
# Whenever I updated my ASP.NET Core source code. It tests the code, packages it up, and deploys it to AKS in seconds.
# Whatever CI/CD tool you use, invest in automating your path to production.
- name: deploy-app
  plan:
  - get: azure-container-registry
    trigger: true
    passed:
    - containerize-app
  - get: source-code
  - get: version
  - put: azure-kubernetes-service
    params:
      # kubectl: get pods
      kubectl: apply -f ((source-code-ci-folder))/deployment.yaml -f ((source-code-ci-folder))/service.yaml
  - put: azure-kubernetes-service
    params:
      kubectl: |
        patch deployment demo-app -p '{"spec":{"template":{"spec":{"containers":[{"name":"demo-app","image":"((azure-container-registry-repository)):'$(cat ((github-git-branch))/((github-git-file)))'"}]}}}}'


# run local powershell
# ====================
# kubectl config get-contexts
# kubectl get secrets
# az aks get-credentials --resource-group VSP_Resource_Group --name vs-k8s-cluster
# kubectl create secret docker-registry acr-auth --docker-server <registry_Login_server> --docker-username <registry_username> --docker-password <registry_password> --docker-email mohideen.kader@titletec.com
# kubectl create secret docker-registry acr-auth --docker-server ftmyrepository.azurecr.io --docker-username FTMyRepository --docker-password 7TwDC=wDsMg0v8NFvARN2juln1mPdREE --docker-email mohideen.kader@titletec.com
# run azure k8s dashboadrd > az aks browse --resource-group VSP_Resource_Group --name VS-k8s-Cluster
#    Proxy running on http://127.0.0.1:8001/
# run azure docker image > docker run ftmyrepository.azurecr.io/seroter-api-k8s:latest

# run azure bash 
# ==============
# to run azure k8s dashboadrd $ az aks browse --resource-group VSP_Resource_Group --name VS-k8s-Cluster
#    To view the console, please open https://gateway11.southcentralus.console.azure.com/n/cc-6a8dba8d/cc-6a8dba8d/proxy/8001/ in a new tab
#

# links
# ======
# https://docs.microsoft.com/en-us/azure/container-registry/container-registry-auth-aks
# https://docs.microsoft.com/en-us/azure/aks/kubernetes-dashboard
# https://supergiant.io/blog/how-to-debug-kubernetes-applications/
# https://concourse-ci.org/
# #!/bin/bash




# m_k@Azure:~$ AKS_RESOURCE_GROUP=VSP_Resource_Group
# m_k@Azure:~$ AKS_CLUSTER_NAME=vs-k8s-cluster
# m_k@Azure:~$ ACR_RESOURCE_GROUP=VSP_Resource_Group
# m_k@Azure:~$ ACR_NAME=FTMyRepository
# m_k@Azure:~$ CLIENT_ID=$(az aks show --resource-group $AKS_RESOURCE_GROUP --name $AKS_CLUSTER_NAME --query "servicePrincipalProfile.clientId" --output tsv)
# m_k@m_k:~$ ACR_ID=$(az acr show --name $ACR_NAME --resource-group $ACR_RESOURCE_GROUP --query "id" --output tsv)
# mohideen_kader@Azure:~$ az role assignment create --assignee $CLIENT_ID --role acrpull --scope $ACR_ID
# {
#   "canDelegate": null,
#   "id": "/subscriptions/1ff34b20-ae5c-4f24-94e2-c5e60f20d1fe/resourceGroups/VSP_Resource_Group/providers/Microsoft.ContainerRegistry/registries/FTMyRepository/providers/Microsoft.Authorization/roleAssignments/49d70675-f177-45a2-8bd1-94cfa75ec9cb",
#   "name": "49d70675-f177-45a2-8bd1-94cfa75ec9cb",
#   "principalId": "e18094f9-d925-4c77-a910-bc212a90b293",
#   "principalName": "api://97c55954-4445-4c30-a104-bf21af88b954",
#   "principalType": "ServicePrincipal",
#   "resourceGroup": "VSP_Resource_Group",
#   "roleDefinitionId": "/subscriptions/1ff34b20-ae5c-4f24-94e2-c5e60f20d1fe/providers/Microsoft.Authorization/roleDefinitions/7f951dda-4ed3-4680-a7ca-43fe172d538d",
#   "roleDefinitionName": "AcrPull",
#   "scope": "/subscriptions/1ff34b20-ae5c-4f24-94e2-c5e60f20d1fe/resourceGroups/VSP_Resource_Group/providers/Microsoft.ContainerRegistry/registries/FTMyRepository",
#   "type": "Microsoft.Authorization/roleAssignments"
# }
# m_k@Azure:~$